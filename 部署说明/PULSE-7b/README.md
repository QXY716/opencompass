## <p align="center">ğŸ“£ PULSE-7bæ¨¡å‹å’ŒMedBenchæ•°æ®é›†éƒ¨ç½²è¿‡ç¨‹</p>

## ä¸€ã€å®˜æ–¹ç½‘é¡µDemoæ–¹å¼éƒ¨ç½²æµç¨‹
- æ‹‰å–PULSE-7bv5æ¨¡å‹
    ```bash
    hf-cli --model-id OpenMEDLab/PULSE-7bv5
    ```
- æ‹‰å–gitä»“åº“å¹¶åˆ›å»ºcondaç¯å¢ƒ
    ```bash
    git clone https://github.com/openmedlab/PULSE.git
    cd PULSE
    conda env create -f llm.yml
    conda activate llm
    ```
- ä¿®æ”¹`web_demo_gradio.py`è„šæœ¬
    <br>è®¾ç½®è¿è¡Œç¯å¢ƒï¼š
    ```python
    parser = argparse.ArgumentParser()
    ## ä¿®æ”¹è·¯å¾„ï¼Œdefault=""ä¸­æ›¿æ¢ä¸ºæœ¬æœºä¸­çš„PULSE-7bv5æ¨¡å‹è·¯å¾„
    parser.add_argument("--model_name", default="OpenMEDLab/PULSE-7bv5", type=str)  
    ## è®¾ç½®GPUæ•°é‡å’Œç¼–å·ï¼Œä¾‹å¦‚è®¾ç½® default="0,1" è¡¨ç¤ºæ¨¡å‹è¿è¡Œåœ¨åŒå¡ä¸Šï¼Œnvidia-smiå‘½ä»¤æŸ¥çœ‹æ˜¾å¡ç¼–å·
    parser.add_argument("--gpu", default="0", type=str)
    parser.add_argument("--input_max_len", default=1536, type=int)
    args = parser.parse_args()
    ```
    <br>è§£å†³PyTorchæ˜¾å­˜ç¢ç‰‡åŒ–é—®é¢˜ï¼š
    ```python
    ## æ·»åŠ ä¸‹è¡Œä»£ç ï¼š
    os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:64"
    ```
    <br>è®¾ç½®batch_sizeå¤§å°ï¼š
    ```python
    ## æ·»åŠ ï¼š
    batch_size = 1
    ```
- æ‰§è¡Œ`python web_demo_gradio.py`å¯åŠ¨æ¨¡å‹

## äºŒã€å°æ˜¾å­˜GPUçš„é‡åŒ–è¿è¡Œæ–¹æ³•
1. huggingfaceå®˜æ–¹æŒ‡å—ä¸­æä¾›çš„é‡åŒ–æ–¹æ³•
<br>[transformersä¸­é›†æˆçš„é‡åŒ–åº“](https://huggingface.co/docs/transformers/quantization)
<br>[é‡åŒ–Transformersæ¨¡å‹](https://huggingface.co/docs/transformers/main/zh/main_classes/quantization)
<br>[ç”¨ bitsandbytesã€4 æ¯”ç‰¹é‡åŒ–å’Œ QLoRA æ‰“é€ äº²æ°‘çš„ LLM](https://huggingface.co/blog/zh/4bit-transformers-bitsandbytes)

2. é€‰æ‹©bitsandbytesåŠ è½½PULSE-7bv5æ¨¡å‹
    <br>ç¡®ä¿åº“çš„å®Œæ•´å®‰è£…ï¼š
    ```python
    conda activate llm
    pip install bitsandbytes>=0.39.0
    pip install --upgrade accelerate
    pip install --upgrade transformers
    ```
    <br>è°ƒæ•´`web_demo_gradio.py`è„šæœ¬ï¼Œè®¾ç½®int4é‡åŒ–ï¼š
    ```python
    ## å¯ç”¨BitsAndBytesConfigåº“
    from transformers import BitsAndBytesConfig
    ```
    ```python
    ## é…ç½®int4é‡åŒ–å‚æ•°
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.bfloat16,
    )

    ## æ¨¡å‹åŠ è½½
    model = AutoModelForCausalLM.from_pretrained(
        args.model_name, 
        torch_dtype=torch.bfloat16,
        quantization_config=bnb_config, ## åŠ è½½é‡åŒ–
        device_map="auto",
    ).eval()
    ```

3. é…ç½®å‚è€ƒ
    <br>![é…ç½®å‚è€ƒ](web_demo.png)
    
    >batch size=1æ—¶æœ¬åœ°éƒ¨ç½²PULSEè¿›è¡Œæ¨ç†æ‰€éœ€çš„æ˜¾å­˜å¤§å°ã€‚
    >|æ¨¡å‹å‚æ•°| é‡åŒ–ç­‰çº§ | åŠ è½½æ¨¡å‹ |
    >| -------- | -------- | -------- |
    >|7B        | FP16     | 14GB     |
    >|7B        | INT4     | 6GB      |

## ä¸‰ã€åœ¨OpenCompassä¸­æ·»åŠ å¹¶è¿è¡Œæ¨ç†çš„è¿‡ç¨‹
### MedBenchæ•°æ®é›†çš„å‡†å¤‡

### PULSE-7bæ·»åŠ æ–¹æ³•

### è¿è¡Œè¯„ä¼°

