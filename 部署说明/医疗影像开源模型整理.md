## <p align="center">📣 医疗影像开源模型 </p>
## 口腔
| 模型名字      |        说明 |          地址 |
|:-------------|:-----------:|--------------:|


## 牙科
| 模型名字      |        说明 |          地址 |
|:-------------|:-----------:|--------------:|

## 面部
| 模型名字      |        说明 |          地址 |
|:-------------|:-----------:|--------------:|
|RETFound|openmedlab视网膜图像的基础模型|https://github.com/openmedlab/RETFound_MAE.git

## 放射类
| 模型名字      |        说明 |          地址 |
|:-------------|:-----------:|--------------:|
|BiomedJourney|通过多模式患者旅程的指导学习生成反事实生物医学图像 |https://microsoft.github.io/BiomedJourney/
|XrayGPT|XrayGPT 旨在激发基于给定的 X 射线对胸部 X 光片进行自动分析的研究。 |https://github.com/mbzuai-oryx/XrayGPT
|RadFM|多模态的放射学基础模型|https://github.com/chaoyi-wu/RadFM
|ChatCAD|使用 LLM 实现可靠且通用的交互式 CAD|https://github.com/zhaozh10/ChatCAD
|CheXagent|胸部 X 光片解读的基础模型|https://github.com/Stanford-AIMI/CheXagent
|LLM-CXR|用于 CXR 图像理解和生成的指令微调 LLM |https://github.com/hyn2028/llm-cxr

## 通用
| 模型名字      |        说明 |          地址 |
|:-------------|:-----------:|--------------:|
|LLaVA-Med|一种大型语言和视觉模型，使用课程学习方法进行训练，以使 LLaVA 适应生物医学领域。|https://github.com/microsoft/LLaVA-Med
|Chinese-LLaVA-Med|中文医学多模态大模型|https://github.com/BUAADreamer/Chinese-LLaVA-Med
|Med-Flamingo|多模态医学小样本学习器|https://github.com/snap-stanford/med-flamingo
|PMC-CLIP|该模型使用对比语言-图像预训练生物医学文档，包括来自 PubMedCentral 的 OpenAccess 子集的 160 万个图像-标题对的数据集。|https://github.com/WeixiongLin/PMC-CLIP/
|PMC-VQA|一种基于生成式的医学视觉理解模型，将来自预先训练的视觉编码器的视觉信息与大型语言模型进行对齐，并建立了一个可扩展的管道来构建大规模医学视觉问答数据集 PMC-VQA，其中包含 149k 张图像的 227k 个 VQA 对，涵盖各种模态或疾病。|https://github.com/xiaoman-zhang/PMC-VQA
|Qilin-Med-VL|旨在整合文本和视觉数据分析的中文大型视觉语言模型。Qilin-Med-VL 将预训练的视觉转换器 (ViT) 与基础 LLM 相结合。它经历了包括特征对齐和指令调整在内的彻底的两阶段课程培训过程。此方法增强了模型生成医学字幕和回答复杂医学查询的能力。|https://github.com/williamliujl/Qilin-Med-VL
|OpenBioMed|一种全新的多模态语义理解框架 BioMedGPT，它运用了生物医学领域中的预训练大语言模型—BioMedGPT-LM作为桥梁，将自然语言、生物编码语言以及化学分子语言等连接起来。|https://github.com/PharMolix/OpenBioMed
|Quilt-LLaVA|能够描述组织病理学贴片中突出的医学区域。此外，它还可用于根据当前观察结果进行诊断。|https://quilt-llava.github.io/
|MedDr|用于大规模医学视觉语言学习的诊断引导引导|https://smart-meddr.github.io/
|Surgical-LVLM|学习适应大型视觉语言模型，用于机器人手术中的基于视觉的问答|https://github.com/gkw0010/Surgical-LVLM